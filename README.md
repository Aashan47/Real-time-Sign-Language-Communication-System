# ğŸ¤Ÿ Real-time Sign Language Communication System

A cross-platform application that enables **real-time, bidirectional communication** between hearing and hearing-impaired individuals through **sign language recognition** and **speech synthesis**.

This system bridges the communication gap by combining **computer vision**, **machine learning**, and **speech technologies** within an intuitive, Flutter-powered user experience.

---

## ğŸš€ Features

- ğŸ”¤ **Real-time Sign Language Detection**  
  Detects hand gestures using machine learning and translates them to text and speech.

- ğŸ—£ï¸ **Speech-to-Text & Text-to-Sign Conversion**  
  Converts voice input to sign language visualizations for better accessibility.

- ğŸ“± **Cross-Platform Support**  
  Built with Flutter to support Android, iOS, Web, Windows, macOS, and Linux.

- ğŸŒ **Ngrok-powered Backend Integration**  
  Python backend uses `ngrok` tunneling for seamless real-time interaction.

---

## ğŸ§  Tech Stack

### Frontend (`equispeak/`)
- [Flutter](https://flutter.dev/) (Dart)
- Platform Support: Android, iOS, Web, Desktop
- Asset-based animations or sign visuals

### Backend (`Backend implementation/`)
- Python 3.x
- Libraries: OpenCV, TensorFlow or MediaPipe (inferred)
- Local server via Flask or FastAPI
- Ngrok for public URL tunneling

---

## ğŸ“ Project Structure

<pre>
â”œâ”€â”€ equispeak/                 # Flutter front-end
â”‚   â”œâ”€â”€ lib/                   # Dart source code
â”‚   â”œâ”€â”€ android/ ios/ web/     # Platform-specific code
â”‚   â”œâ”€â”€ assets/                # Images, animations, videos
â”‚   â””â”€â”€ pubspec.yaml           # Flutter dependencies
â”‚
â”œâ”€â”€ Backend implementation/    # Python-based ML backend
â”‚   â”œâ”€â”€ models/                # Pretrained or custom ML models
â”‚   â”œâ”€â”€ src/                   # Backend utilities and helpers
â”‚   â”œâ”€â”€ main.py / server.py    # API endpoints and inference logic
â”‚   â””â”€â”€ requirements.txt       # Python dependencies
</pre>

---

## âš™ï¸ Getting Started

### ğŸ”§ Frontend Setup

```bash
cd equispeak
flutter pub get
flutter run
```

### ğŸ§  Backend Setup

```bash
cd "Backend implementation"
pip install -r requirements.txt
python server.py
```

> To expose the backend using ngrok:

```bash
ngrok http 5000
```

---

## ğŸ“Œ Use Cases

This system is ideal for:

- Educational institutions  
- Customer service counters  
- Healthcare environments  
- Public-facing offices  
- One-on-one communication  

Anywhere sign language users need **instant understanding**, this app ensures **inclusive communication**.

---

## ğŸ‘¤ Author

**Aashan Javed**  
GitHub: [@Aashan47](https://github.com/Aashan47)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ› ï¸ Future Improvements (Suggestions)

- Support for multiple sign languages (e.g., BSL, ISL)  
- Improved training using real-world gesture datasets  
- Offline mode via on-device ML  
- Customizable avatars or visual output styles  

---
